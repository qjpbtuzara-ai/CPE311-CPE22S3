{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+y5kllWLLwqNdkJ3BrqJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qjpbtuzara-ai/CPE311-CPE22S3/blob/main/Seatwork7_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Tuzara, John Paul B. <br>\n",
        "Course & Section: CPE311-CPE22S3 <br>\n",
        "Date: 2/19/26 <br>\n",
        "Instructor: Engr. Neal Barton James Matira"
      ],
      "metadata": {
        "id": "5eSulHpnJlKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise Part 4:\n",
        "\n",
        "    Using the meteorite data from the Meteorite_Landings.csv file, create a pivot table that shows both the number of meteorites and the 95th percentile of meteorite mass for those that were found versus observed falling per year from 2005 through 2009 (inclusive). Hint: Be sure to convert the year column to a number as we did in the previous exercise.\n",
        "    Using the meteorite data from the Meteorite_Landings.csv file, compare summary statistics of the mass column for the meteorites that were found versus observed falling.\n"
      ],
      "metadata": {
        "id": "0dxDCDC3shVv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yL4N821xIrZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9ef2b8-8c5e-4b5b-b512-726bf108964e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Pivot Table: 2005-2009 ---\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "\n",
            "--- Summary Statistics: Found vs Fell ---\n",
            "         count          mean            std  min     25%     50%      75%  \\\n",
            "fall                                                                        \n",
            "Fell    1075.0  47070.715023  717067.125826  0.1  686.00  2800.0  10450.0   \n",
            "Found  44510.0  12461.922983  571105.752311  0.0    6.94    30.5    178.0   \n",
            "\n",
            "              max  \n",
            "fall               \n",
            "Fell   23000000.0  \n",
            "Found  60000000.0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "meteorites = pd.read_csv('Meteorite_Landings.csv')\n",
        "\n",
        "# Year column to numeric\n",
        "meteorites['year'] = pd.to_numeric(meteorites['year'], errors='coerce')\n",
        "\n",
        "# Filter for meteorites from 2005 through 2009\n",
        "meteorite_filter = meteorites.query('year >= 2005 and year <= 2009').copy()\n",
        "\n",
        "# Create the pivot table\n",
        "pivot_table = meteorite_filter.pivot_table(\n",
        "    index='year',\n",
        "    columns='fall',\n",
        "    values='mass (g)',\n",
        "    aggfunc=['count', lambda x: x.quantile(0.95)]\n",
        ").rename(columns={'<lambda>': '95th percentile'})\n",
        "\n",
        "print(\"--- Pivot Table: 2005-2009 ---\")\n",
        "print(pivot_table)\n",
        "\n",
        "#  Compare summary statistics for the mass column\n",
        "mass_comparison = meteorites.groupby('fall')['mass (g)'].describe()\n",
        "\n",
        "print(\"\\n--- Summary Statistics: Found vs Fell ---\")\n",
        "print(mass_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The pivot table that was created via describe() method allowed me to compare the frequency of meteorites that were found instead of falling annually in which case found meteorites are much more common. It has highlighted the differences in the mean, std, and the max mass between meteorites of those seen falling and then those discovered much later. This also means that at the 95th percentile, certain years had exceptionally large meteorites compared to the standard average. I have observed that by 2005-2009 there were spikes in the meteors that fell."
      ],
      "metadata": {
        "id": "Y_aYPdyhySue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise Part 5:\n",
        "Using the taxi trip data in the 2019_Yellow_Taxi_Trip_Data.csv file, resample the data to an hourly frequency based on the dropoff time. Calculate the total trip_distance, fare_amount, tolls_amount, and tip_amount, then find the 5 hours with the most tips."
      ],
      "metadata": {
        "id": "59Fxv492suaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "taxi = pd.read_csv('2019_Yellow_Taxi_Trip_Data.csv')\n",
        "\n",
        "#Dropoff time to datetime objects\n",
        "taxi['tpep_dropoff_datetime'] = pd.to_datetime(taxi['tpep_dropoff_datetime'])\n",
        "\n",
        "#Resample to hourly frequency based on dropoff time\n",
        "hourly_taxi = taxi.set_index('tpep_dropoff_datetime').resample('H')[[\n",
        "    'trip_distance',\n",
        "    'fare_amount',\n",
        "    'tolls_amount',\n",
        "    'tip_amount'\n",
        "]].sum()\n",
        "\n",
        "#Find the 5 hours with the most tips\n",
        "top_5_tip_hours = hourly_taxi.nlargest(5, 'tip_amount')\n",
        "\n",
        "print(\"--- Top 5 Hours with Highest Total Tips ---\")\n",
        "print(top_5_tip_hours)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxX7ivogv1Th",
        "outputId": "776725c4-96a4-4be6-b68a-8b8ddc36a561"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Top 5 Hours with Highest Total Tips ---\n",
            "                       trip_distance  fare_amount  tolls_amount  tip_amount\n",
            "tpep_dropoff_datetime                                                      \n",
            "2019-10-23 16:00:00         10676.95     67797.76        699.04    12228.64\n",
            "2019-10-23 17:00:00         16052.83     70131.91       4044.04    12044.03\n",
            "2019-10-23 18:00:00          3104.56     11565.56       1454.67     1907.64\n",
            "2019-10-23 15:00:00            14.34       213.50          0.00       51.75\n",
            "2019-10-23 19:00:00            98.59       268.00         24.48       25.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-153106389.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  hourly_taxi = taxi.set_index('tpep_dropoff_datetime').resample('H')[[\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: I have observed that by resampling by hour shows when the day generate the highest total fare and toll revenue as it aligns with rush hours or late-night shift. The hours shows that hours with the most tips help point the most lucrative time for drivers and their frequency corresponds with the taxi demand that the fleet is most active based on drop off timestamps. I also observed how trip distance affects the earnings throughout the day."
      ],
      "metadata": {
        "id": "LSP8HdXO0O99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "I have learned more about interpreting data from csv files into tables using the pandas module from python and getting them to execute properly such as in type casting and coercing the errors. In example 4, most meteorites in the dataset are found rather than that they fell in which case they are discovered way before any witnesses. Creating a pivot table for 2005 to 2009 shows wether meteorite discoveries are consistent year to year. In example 5, resampling data to an hourly frequency shows how taxi revenue and usage arent spread as evenly across the day and that identifying the top 5 hours would mean that there are times that it would be profitable for drivers than others. Pivot tables and resampling are essential to specify large datasets into easy to read summaries."
      ],
      "metadata": {
        "id": "dPuZHc9m1m7z"
      }
    }
  ]
}